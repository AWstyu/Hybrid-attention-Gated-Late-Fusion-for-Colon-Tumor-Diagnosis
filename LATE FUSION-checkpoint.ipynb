{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e233d32-591d-44e5-ba71-17371a2534ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\COLON_CANCER DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d2bfb-7672-4db8-931f-79d8a710d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"shape_features_labels.xlsx\"   \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "class_0_count = (df[\"Agglomerative_Label\"] == 0).sum()\n",
    "class_1_count = (df[\"Agglomerative_Label\"] == 1).sum()\n",
    "\n",
    "print(f\"Number of class 0 data points: {class_0_count}\")\n",
    "print(f\"Number of class 1 data points: {class_1_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600cf4d-6022-4ad5-970d-a9a7e18f4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"texture_features_labels.xlsx\"   \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "class_0_count = (df[\"Agglomerative_Label\"] == 0).sum()\n",
    "class_1_count = (df[\"Agglomerative_Label\"] == 1).sum()\n",
    "\n",
    "print(f\"Number of class 0 data points: {class_0_count}\")\n",
    "print(f\"Number of class 1 data points: {class_1_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b3482-ae30-4115-b669-05805f72424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "shape_df = pd.read_excel(\"shape_features_labels.xlsx\")\n",
    "texture_df = pd.read_excel(\"texture_features_labels.xlsx\")\n",
    "\n",
    "\n",
    "merged_df = pd.concat([shape_df.reset_index(drop=True), \n",
    "                       texture_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "merged_df.to_excel(\"merged_shape_texture.xlsx\", index=False)\n",
    "\n",
    "print(f\"Merged shape: {merged_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403b67d-9911-45ca-8079-2500c5326f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "exclude_cols = [\"Image_Name\", \"euler_number\"]\n",
    "features_df = df.drop(columns=[col for col in exclude_cols if col in df.columns])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_df)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(features_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(features_2d[:,0], features_2d[:,1], s=40, alpha=0.7)\n",
    "plt.title(\"Scatter Plot of All Normalized Features (PCA 2D)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed6658-47da-4ac4-9bf4-1a2e1d7963a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "exclude_cols = [\"Image_Name\", \"euler_number\"]\n",
    "features_df = df.drop(columns=[col for col in exclude_cols if col in df.columns])\n",
    "\n",
    "\n",
    "image_names = df[\"Image_Name\"] if \"Image_Name\" in df.columns else None\n",
    "\n",
    "\n",
    "feature_vectors = features_df.values\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feature_vectors_scaled = scaler.fit_transform(feature_vectors)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_result = pca.fit_transform(feature_vectors_scaled)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne.fit_transform(feature_vectors_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_result[:,0], pca_result[:,1], s=40, alpha=0.7)\n",
    "plt.title(\"PCA (2D) on Features\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(tsne_result[:,0], tsne_result[:,1], s=40, alpha=0.7)\n",
    "plt.title(\"t-SNE (2D) on Features\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a6ab8-9f47-47c1-afdd-45dfac40194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"  \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_2d = pca.fit_transform(features)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_2d = tsne.fit_transform(features)\n",
    "\n",
    "\n",
    "def apply_kmeans(data, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    sil = silhouette_score(X, labels)\n",
    "    ch = calinski_harabasz_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "\n",
    "n_clusters = 2\n",
    "\n",
    "\n",
    "labels_pca2D = apply_kmeans(pca_2d, n_clusters)\n",
    "sil_pca2D, ch_pca2D, db_pca2D = evaluate_clustering(pca_2d, labels_pca2D)\n",
    "\n",
    "\n",
    "labels_tsne2D = apply_kmeans(tsne_2d, n_clusters)\n",
    "sil_tsne2D, ch_tsne2D, db_tsne2D = evaluate_clustering(tsne_2d, labels_tsne2D)\n",
    "\n",
    "\n",
    "print(\"\\nClustering Quality Comparison:\\n\")\n",
    "print(f\"{'Projection':<15} {'Silhouette':>12} {'CH Index':>12} {'DB Index':>12}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'PCA 2D':<15} {sil_pca2D:12.4f} {ch_pca2D:12.2f} {db_pca2D:12.4f}\")\n",
    "print(f\"{'t-SNE 2D':<15} {sil_tsne2D:12.4f} {ch_tsne2D:12.2f} {db_tsne2D:12.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=labels_pca2D, cmap='viridis')\n",
    "plt.title('PCA 2D Clustering')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(tsne_2d[:, 0], tsne_2d[:, 1], c=labels_tsne2D, cmap='plasma')\n",
    "plt.title('t-SNE 2D Clustering')\n",
    "plt.xlabel('Dim 1')\n",
    "plt.ylabel('Dim 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5748e2-a8a7-4884-b663-8f40b8ede893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift, OPTICS, Birch\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(features)\n",
    "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(features)\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    if len(set(labels)) < 2:  \n",
    "        return None, None, None\n",
    "    sil = silhouette_score(X, labels)\n",
    "    ch = calinski_harabasz_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "\n",
    "def plot_clusters(X, labels, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=\"tab10\", s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "algo = AgglomerativeClustering(n_clusters=2)\n",
    "\n",
    "for method_name, data in [(\"PCA\", pca_2d), (\"t-SNE\", tsne_2d)]:\n",
    "    labels = algo.fit_predict(data)\n",
    "    sil, ch, db = evaluate_clustering(data, labels)\n",
    "    print(f\"Agglomerative | {method_name} | Silhouette: {sil} | CH: {ch} | DB: {db}\")\n",
    "    plot_clusters(data, labels, f\"Agglomerative ({method_name})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2efd8-f349-4337-8a3f-9c6d0f69799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift, OPTICS, Birch\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(features)\n",
    "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(features)\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    if len(set(labels)) < 2:\n",
    "        return None, None, None\n",
    "    sil = silhouette_score(X, labels)\n",
    "    ch = calinski_harabasz_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "\n",
    "def plot_clusters(X, labels, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=\"tab10\", s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "algo = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "for method_name, data in [(\"PCA\", pca_2d), (\"t-SNE\", tsne_2d)]:\n",
    "    labels = algo.fit_predict(data)\n",
    "    sil, ch, db = evaluate_clustering(data, labels)\n",
    "    print(f\"DBSCAN | {method_name} | Silhouette: {sil} | CH: {ch} | DB: {db}\")\n",
    "    plot_clusters(data, labels, f\"DBSCAN ({method_name})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08e50b-dabb-4a9e-b3ab-f24dbdcc23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift, OPTICS, Birch\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(features)\n",
    "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(features)\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    if len(set(labels)) < 2:\n",
    "        return None, None, None\n",
    "    sil = silhouette_score(X, labels)\n",
    "    ch = calinski_harabasz_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "\n",
    "def plot_clusters(X, labels, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=\"tab10\", s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "algo = MeanShift()\n",
    "\n",
    "for method_name, data in [(\"PCA\", pca_2d), (\"t-SNE\", tsne_2d)]:\n",
    "    labels = algo.fit_predict(data)\n",
    "    sil, ch, db = evaluate_clustering(data, labels)\n",
    "    print(f\"MeanShift | {method_name} | Silhouette: {sil} | CH: {ch} | DB: {db}\")\n",
    "    plot_clusters(data, labels, f\"MeanShift ({method_name})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a4cd1-1970-4135-af25-e3c8a46e38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift, OPTICS, Birch\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(features)\n",
    "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(features)\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    if len(set(labels)) < 2:\n",
    "        return None, None, None\n",
    "    sil = silhouette_score(X, labels)\n",
    "    ch = calinski_harabasz_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "def plot_clusters(X, labels, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=\"tab10\", s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "algo = OPTICS()\n",
    "\n",
    "for method_name, data in [(\"PCA\", pca_2d), (\"t-SNE\", tsne_2d)]:\n",
    "    labels = algo.fit_predict(data)\n",
    "    sil, ch, db = evaluate_clustering(data, labels)\n",
    "    print(f\"OPTICS | {method_name} | Silhouette: {sil} | CH: {ch} | DB: {db}\")\n",
    "    plot_clusters(data, labels, f\"OPTICS ({method_name})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6331ef9-5b62-42ec-bf1d-9bcdac673629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift, OPTICS, Birch\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(features)\n",
    "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(features)\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    if len(set(labels)) < 2:\n",
    "        return None, None, None\n",
    "    sil = silhouette_score(X, labels)\n",
    "    ch = calinski_harabasz_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "\n",
    "def plot_clusters(X, labels, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=\"tab10\", s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "algo = Birch(n_clusters=2)\n",
    "\n",
    "for method_name, data in [(\"PCA\", pca_2d), (\"t-SNE\", tsne_2d)]:\n",
    "    labels = algo.fit_predict(data)\n",
    "    sil, ch, db = evaluate_clustering(data, labels)\n",
    "    print(f\"Birch | {method_name} | Silhouette: {sil} | CH: {ch} | DB: {db}\")\n",
    "    plot_clusters(data, labels, f\"Birch ({method_name})\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c33951-0561-4c37-a5e7-84e760df5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(features)\n",
    "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(features)\n",
    "\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    if len(set(labels)) < 2:\n",
    "        return None, None, None\n",
    "    sil = silhouette_score(X, labels)\n",
    "    ch = calinski_harabasz_score(X, labels)\n",
    "    db = davies_bouldin_score(X, labels)\n",
    "    return sil, ch, db\n",
    "\n",
    "\n",
    "def plot_clusters(X, labels, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap=\"tab10\", s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "algo = GaussianMixture(n_components=2, random_state=42)\n",
    "\n",
    "for method_name, data in [(\"PCA\", pca_2d), (\"t-SNE\", tsne_2d)]:\n",
    "    labels = algo.fit_predict(data)\n",
    "    sil, ch, db = evaluate_clustering(data, labels)\n",
    "    print(f\"GMM | {method_name} | Silhouette: {sil} | CH: {ch} | DB: {db}\")\n",
    "    plot_clusters(data, labels, f\"GMM ({method_name})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf3cec-c031-44af-ba81-063a1752bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering, Birch,KMeans\n",
    "\n",
    "\n",
    "file_path = \"merged_shape_texture.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "image_names = df.iloc[:, 0]\n",
    "features = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2).fit_transform(features)\n",
    "\n",
    "def apply_kmeans(data, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "n_clusters = 2\n",
    "\n",
    "\n",
    "kmeans_labels= apply_kmeans(pca_2d, n_clusters)\n",
    "agg_labels = AgglomerativeClustering(n_clusters=2).fit_predict(pca_2d)\n",
    "birch_labels = Birch(n_clusters=2).fit_predict(pca_2d)\n",
    "\n",
    "\n",
    "labels_df = pd.DataFrame({\n",
    "    \"Image_Name\": image_names,\n",
    "    \"Kmeans_Label\":kmeans_labels,\n",
    "    \"Agglomerative_Label\": agg_labels,\n",
    "    \"Birch_Label\": birch_labels\n",
    "})\n",
    "\n",
    "\n",
    "output_path = \"clustering_labels_for_merged_features.xlsx\"\n",
    "labels_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Clustering labels saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af952aa-fefe-43ff-814e-bb98dd51bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features_file = \"merged_shape_texture.xlsx\"     \n",
    "labels_file = \"clustering_labels_for_merged_features.xlsx\"          \n",
    "\n",
    "\n",
    "df_features = pd.read_excel(features_file)\n",
    "df_labels = pd.read_excel(labels_file)\n",
    "\n",
    "\n",
    "df_features.rename(columns={\"Image Name\": \"Image_Name\"}, inplace=True)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(df_features, df_labels, on=\"Image_Name\", how=\"inner\")\n",
    "\n",
    "\n",
    "output_file = \"merged_features_labels.xlsx\"\n",
    "merged_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Merged file saved as {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a1747-9c7a-4d8b-bc6d-35ab098390b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"clustering_labels_for_merged_features.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "def print_cluster_distribution(labels, method_name):\n",
    "    counts = labels.value_counts().sort_index()           \n",
    "    ratios = labels.value_counts(normalize=True).sort_index() * 100  \n",
    "    print(f\"\\n{method_name} Cluster Distribution:\")\n",
    "    for cluster in counts.index:\n",
    "        print(f\"Cluster {cluster}: {counts[cluster]} samples ({ratios[cluster]:.2f}%)\")\n",
    "\n",
    "\n",
    "print_cluster_distribution(df[\"Kmeans_Label\"], \"KMeans\")\n",
    "print_cluster_distribution(df[\"Agglomerative_Label\"], \"Agglomerative\")\n",
    "print_cluster_distribution(df[\"Birch_Label\"], \"Birch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4045143-7cf1-4469-91bf-d9c8fe6cd331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = \"merged_features_labels.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "df[\"Image_Name\"] = (\n",
    "    df[\"Image_Name\"]\n",
    "    .str.replace(\"_mask\", \"\", regex=False)\n",
    "    .str.replace(\".jpg\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "\n",
    "output_path = \"merged_features_labels_cleaned.xlsx\"\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned image names saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0ba9a-9611-4010-a6d3-f946f52bba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"merged_features_labels_cleaned.xlsx\"   \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "X = df.drop(columns=[\"Image_Name\",\"Kmeans_Label\", \"Agglomerative_Label\" ,\"Birch_Label\"]).values\n",
    "y = df[\"Kmeans_Label\"].values\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# print(\"Before SMOTE:\", np.bincount(y))\n",
    "# print(\"After SMOTE :\", np.bincount(y_res))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds  = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train.numpy()),\n",
    "    y=y_train.numpy()\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "\n",
    "class KANLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.2):\n",
    "        super(KANLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.attn = nn.Linear(out_dim, 1)\n",
    "        self.norm = nn.LayerNorm(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.linear(x)  \n",
    "        attn_scores = torch.matmul(h, h.T)  \n",
    "        attn_scores = F.softmax(attn_scores, dim=1)\n",
    "        h_new = torch.matmul(attn_scores, h)  \n",
    "        h_new = F.relu(h_new)\n",
    "        h_new = self.dropout(h_new)\n",
    "        return self.norm(h_new + h)  \n",
    "\n",
    "\n",
    "class CustomKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3):\n",
    "        super(CustomKAN, self).__init__()\n",
    "\n",
    "        self.kan_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)):\n",
    "            in_dim = input_dim if i == 0 else hidden_dims[i-1]\n",
    "            out_dim = hidden_dims[i]\n",
    "            self.kan_layers.append(KANLayer(in_dim, out_dim, dropout))\n",
    "\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], hidden_dims[-1]*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dims[-1]*2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for layer in self.kan_layers:\n",
    "            h = layer(h)\n",
    "        out = self.mlp_head(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [128, 64, 32]   \n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "model = CustomKAN(input_dim, hidden_dims, num_classes, dropout=0.3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, min_delta=1e-4, save_path=\"best_model.pth\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.save_path)  \n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "early_stopping = EarlyStopping(patience=20, min_delta=1e-4, save_path=\"best_model.pth\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "   \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "   \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item()\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_acc = correct / total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc*100:.2f}% | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    \n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1, len(val_accuracies)+1), val_accuracies, label='Val Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85881d-5e7e-4a99-a3cd-1fecac21a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "X_all = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_all = y  \n",
    "image_names = df[\"Image_Name\"].values  \n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_all)\n",
    "    predicted = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Image_Name\": image_names,\n",
    "    \"True_Label\": y_all,\n",
    "    \"Predicted_Label\": predicted\n",
    "})\n",
    "\n",
    "\n",
    "for c in range(probabilities.shape[1]):\n",
    "    results_df[f\"Prob_Class{c}\"] = probabilities[:, c]\n",
    "\n",
    "results_df.to_csv(\"KAN_predictions_with_probs.csv\", index=False)\n",
    "print(\"Predictions with probabilities saved to KAN_predictions_with_probs.csv\")\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_all, predicted, digits=4))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_all, predicted)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=np.unique(y_all),\n",
    "            yticklabels=np.unique(y_all))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix(KAN)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91497024-266c-4ffc-bb99-8c5fc762a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_caps_in, dim_caps_in, num_caps_out, dim_caps_out, num_routes):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_caps_in = num_caps_in\n",
    "        self.dim_caps_in = dim_caps_in\n",
    "        self.num_caps_out = num_caps_out\n",
    "        self.dim_caps_out = dim_caps_out\n",
    "        self.num_routes = num_routes\n",
    "        self.W = nn.Parameter(0.01 * torch.randn(1, num_caps_in, num_caps_out, dim_caps_out, dim_caps_in))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(2).unsqueeze(4)  \n",
    "        u_hat = torch.matmul(self.W, x).squeeze(-1)        \n",
    "        u_hat = u_hat.expand(batch_size, -1, -1, -1)  \n",
    "        b_ij = torch.zeros(batch_size, self.num_caps_in, self.num_caps_out, 1, device=x.device)\n",
    "\n",
    "        for r in range(self.num_routes):\n",
    "            c_ij = F.softmax(b_ij, dim=2)\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            if r < self.num_routes - 1:\n",
    "                b_ij = b_ij + (u_hat * v_j).sum(-1, keepdim=True)\n",
    "        return v_j.squeeze(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s, dim=-1):\n",
    "        norm = torch.norm(s, p=2, dim=dim, keepdim=True)\n",
    "        scale = (norm**2) / (1 + norm**2)\n",
    "        return scale * s / (norm + 1e-8)\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128], num_classes=2, num_routes=3, dropout=0.4):\n",
    "        super(CapsNet, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        self.num_primary_caps = 16\n",
    "        self.dim_primary_caps = 16\n",
    "        self.primary_caps = nn.Linear(hidden_dims[-1], self.num_primary_caps * self.dim_primary_caps)\n",
    "\n",
    "        self.digit_caps = CapsuleLayer(\n",
    "            num_caps_in=self.num_primary_caps,\n",
    "            dim_caps_in=self.dim_primary_caps,\n",
    "            num_caps_out=num_classes,\n",
    "            dim_caps_out=16,\n",
    "            num_routes=num_routes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        primary_caps = self.primary_caps(x)\n",
    "        primary_caps = primary_caps.view(x.size(0), self.num_primary_caps, self.dim_primary_caps)\n",
    "        digit_caps = self.digit_caps(primary_caps)\n",
    "        logits = torch.norm(digit_caps, dim=-1)  \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return preds, logits\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"merged_features_labels_cleaned.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "X = df.drop(columns=[\"Image_Name\",\"Kmeans_Label\", \"Agglomerative_Label\", \"Birch_Label\"]).values\n",
    "y = df[\"Kmeans_Label\"].values\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# print(\"Before SMOTE:\", np.bincount(y))\n",
    "# print(\"After SMOTE :\", np.bincount(y_res))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CapsNet(input_dim=X_train.shape[1]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 35\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_state = None\n",
    "counter = 0\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"lr\": []}\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds, logits = model(xb)\n",
    "        loss = criterion(logits, yb) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds, logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss Curve\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history[\"val_acc\"], label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Validation Accuracy\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6294dd9-1866-456d-9fd8-6f1110edc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "X_all = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_all = np.array(y)   \n",
    "image_names = df[\"Image_Name\"].values  \n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_all)   \n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[1]  \n",
    "    predicted = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Image_Name\": image_names,\n",
    "    \"True_Label\": y_all,\n",
    "    \"Predicted_Label\": predicted\n",
    "})\n",
    "\n",
    "\n",
    "for c in range(probabilities.shape[1]):\n",
    "    results_df[f\"Prob_Class{c}\"] = probabilities[:, c]\n",
    "\n",
    "results_df.to_csv(\"CAPSNET_predictions_with_probs.csv\", index=False)\n",
    "print(\"Predictions with probabilities saved to 'CAPSNET_predictions_with_probs.csv'\")\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_all, predicted, digits=4))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_all, predicted)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=np.unique(y_all),\n",
    "            yticklabels=np.unique(y_all))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix(CAPSNET)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dcbae-5081-4b3f-ab4b-208b8b8aa58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ComplexAttentionFusion(nn.Module):\n",
    "    def __init__(self, num_classes=2, hidden_dim=64, num_heads=4):\n",
    "        super(ComplexAttentionFusion, self).__init__()\n",
    "        self.input_dim = num_classes * 2  \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        \n",
    "        self.fc_shared = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "       \n",
    "        attn_layers = []\n",
    "        in_dim = hidden_dim\n",
    "        for _ in range(8):  \n",
    "            attn_layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            attn_layers.append(nn.ReLU())\n",
    "            attn_layers.append(nn.Dropout(0.1))\n",
    "        self.attn_mlp = nn.Sequential(*attn_layers)\n",
    "\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, 2) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_classes, 64),   \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_kan, x_caps):\n",
    "        \n",
    "        x = torch.cat([x_kan, x_caps], dim=1)\n",
    "\n",
    "        \n",
    "        h = self.fc_shared(x)\n",
    "\n",
    "        \n",
    "        h_attn = self.attn_mlp(h)\n",
    "\n",
    "        \n",
    "        attn_all = []\n",
    "        for head in self.heads:\n",
    "            attn = torch.softmax(head(h_attn), dim=1)\n",
    "            attn_all.append(attn)\n",
    "        attn_weights = torch.stack(attn_all, dim=0).mean(dim=0)\n",
    "\n",
    "        \n",
    "        w_kan = attn_weights[:, 0].unsqueeze(1)\n",
    "        w_caps = attn_weights[:, 1].unsqueeze(1)\n",
    "        fused = w_kan * x_kan + w_caps * x_caps\n",
    "\n",
    "        \n",
    "        gate = self.gate(x)\n",
    "        fused = gate * fused + (1 - gate) * (0.5 * (x_kan + x_caps))\n",
    "\n",
    "        \n",
    "        out = self.classifier(fused)\n",
    "        return fused, out, attn_weights\n",
    "\n",
    "\n",
    "\n",
    "df_kan = pd.read_csv(\"KAN_predictions_with_probs.csv\")\n",
    "df_caps = pd.read_csv(\"CapsNet_predictions_with_probs.csv\")\n",
    "\n",
    "image_names = df_kan[\"Image_Name\"].values\n",
    "y_true = df_kan[\"True_Label\"].values\n",
    "\n",
    "probs_kan = df_kan[[\"Prob_Class0\", \"Prob_Class1\"]].values\n",
    "probs_caps = df_caps[[\"Prob_Class0\", \"Prob_Class1\"]].values\n",
    "\n",
    "\n",
    "X_train_kan, X_val_kan, X_train_caps, X_val_caps, y_train, y_val = train_test_split(\n",
    "    probs_kan, probs_caps, y_true, test_size=0.2, random_state=42, stratify=y_true\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_kan = torch.tensor(X_train_kan, dtype=torch.float32).to(device)\n",
    "X_train_caps = torch.tensor(X_train_caps, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_val_kan = torch.tensor(X_val_kan, dtype=torch.float32).to(device)\n",
    "X_val_caps = torch.tensor(X_val_caps, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "model = ComplexAttentionFusion(num_classes=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 500\n",
    "patience = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": [], \"attn_kan\": [], \"attn_caps\": []}\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _, out_train, _ = model(X_train_kan, X_train_caps)\n",
    "    loss = criterion(out_train, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, out_val, attn_val = model(X_val_kan, X_val_caps)\n",
    "        val_loss = criterion(out_val, y_val).item()\n",
    "        preds = torch.argmax(out_val, dim=1)\n",
    "        acc = (preds == y_val).float().mean().item()\n",
    "\n",
    "    attn_mean_kan = attn_val[:, 0].mean().item()\n",
    "    attn_mean_caps = attn_val[:, 1].mean().item()\n",
    "\n",
    "    \n",
    "    history[\"train_loss\"].append(loss.item())\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(acc)\n",
    "    history[\"attn_kan\"].append(attn_mean_kan)\n",
    "    history[\"attn_caps\"].append(attn_mean_caps)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{num_epochs} | \"\n",
    "        f\"Train Loss: {loss.item():.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {acc:.4f} | Attn_KAN: {attn_mean_kan:.4f} | Attn_Caps: {attn_mean_caps:.4f}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        counter = 0\n",
    "        torch.save(best_model_state, \"best_attention_fusion.pt\")  \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, out_val, attn_val = model(X_val_kan, X_val_caps)\n",
    "    y_pred_val = torch.argmax(out_val, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"\\n=== Classification Report (Validation Set) ===\\n\")\n",
    "print(classification_report(y_val.cpu().numpy(), y_pred_val, digits=4))\n",
    "\n",
    "cm_val = confusion_matrix(y_val.cpu().numpy(), y_pred_val)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_val, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=np.unique(y_true),\n",
    "            yticklabels=np.unique(y_true))\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Validation Set)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X_all_kan = torch.tensor(probs_kan, dtype=torch.float32).to(device)\n",
    "X_all_caps = torch.tensor(probs_caps, dtype=torch.float32).to(device)\n",
    "y_all = torch.tensor(y_true, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, out_all, attn_all = model(X_all_kan, X_all_caps)\n",
    "    y_all_pred = torch.argmax(out_all, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"\\n=== Classification Report (Whole Dataset) ===\\n\")\n",
    "print(classification_report(y_all.cpu().numpy(), y_all_pred, digits=4))\n",
    "\n",
    "cm_all = confusion_matrix(y_all.cpu().numpy(), y_all_pred)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss Curve\")\n",
    "\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(history[\"val_acc\"], label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Accuracy Curve\")\n",
    "\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.plot(history[\"attn_kan\"], label=\"KAN Weight\")\n",
    "plt.plot(history[\"attn_caps\"], label=\"CapsNet Weight\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Attention Weight\"); plt.legend(); plt.title(\"Attention Weights\")\n",
    "\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "sns.heatmap(cm_all, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "            xticklabels=np.unique(y_true),\n",
    "            yticklabels=np.unique(y_true))\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix (Whole Dataset)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cm_all_norm = cm_all.astype(\"float\") / cm_all.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_all_norm, annot=True, fmt=\".2f\", cmap=\"Oranges\",\n",
    "            xticklabels=np.unique(y_true),\n",
    "            yticklabels=np.unique(y_true))\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.title(\"Normalized Confusion Matrix (Whole Dataset)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9009e-2bd8-430b-9857-745a61023667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
